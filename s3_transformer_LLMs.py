# -*- coding: utf-8 -*-
"""S3_transformer_LLMs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Uaj5UNkqUhNLi0AdSki8i5txtqOKdGt
"""

pip install transformers>=4.41.2 accelerate>=0.31.0

"""*Loading the LLM*"""

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cuda",
    torch_dtype="auto",
    trust_remote_code=True,
)

# Create a pipeline
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=50,
    do_sample=False,
)

"""*The Inputs and Outputs of a Trained Transformer LLM*"""

prompt = "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened."

output = generator(prompt)

print(output[0]['generated_text'])

print(model)

"""*Choosing a single token from the probability distribution (sampling / decoding)*"""

prompt = "The capital of France is"

# Tokenize the input prompt
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# Tokenize the input prompt
input_ids = input_ids.to("cuda")

# Get the output of the model before the lm_head
model_output = model.model(input_ids)

# Get the output of the lm_head
lm_head_output = model.lm_head(model_output[0])

token_id = lm_head_output[0,-1].argmax(-1)
tokenizer.decode(token_id)

model_output[0].shape

lm_head_output.shape

"""*Speeding up generation by caching keys and values*"""

prompt = "Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened."

# Tokenize the input prompt
input_ids = tokenizer(prompt, return_tensors="pt").input_ids
input_ids = input_ids.to("cuda")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit -n 1
# # Generate the text
# generation_output = model.generate(
#   input_ids=input_ids,
#   max_new_tokens=100,
#   use_cache=True
# )

# Commented out IPython magic to ensure Python compatibility.
# %%timeit -n 1
# # Generate the text
# generation_output = model.generate(
#   input_ids=input_ids,
#   max_new_tokens=100,
#   use_cache=False
# )