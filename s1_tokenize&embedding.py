# -*- coding: utf-8 -*-
"""S1_tokenize&embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fU-6EmTldiRccxSqCXsGiVJCJFvTQYgY
"""

pip install transformers>=4.41.2 sentence-transformers>=3.0.1 gensim>=4.3.2 scikit-learn>=1.5.0 accelerate>=0.31.0

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cuda",
    torch_dtype="auto",
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

prompt = "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>"

# Tokenize the input prompt
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")

# Generate the text
generation_output = model.generate(
  input_ids=input_ids,
  max_new_tokens=20
)

# Print the output
print("\n", tokenizer.decode(generation_output[0]))

print(input_ids)

for id in input_ids[0]:
   print(tokenizer.decode(id))

print(generation_output[0])

print(tokenizer.decode(3323))
print(tokenizer.decode(622))
print(tokenizer.decode([3323, 622]))
print(tokenizer.decode(29901))
print(tokenizer.decode(5281))

"""**Comparing Trained LLM Tokenizers**"""

from transformers import AutoModelForCausalLM, AutoTokenizer

colors_list = [
    '102;194;165', '252;141;98', '141;160;203',
    '231;138;195', '166;216;84', '255;217;47'
]

def show_tokens(sentence, tokenizer_name):
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    token_ids = tokenizer(sentence).input_ids
    for idx, t in enumerate(token_ids):
        print(
            f'\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +
            tokenizer.decode(t) +
            '\x1b[0m',
            end=' '
        )

text = """
English and CAPITALIZATION
ðŸŽµ é¸Ÿ
show_tokens False None elif == >= else: two tabs:"    " Three tabs: "       "
12.0*50=600
"""

"""*BERT base model*"""

show_tokens(text, "bert-base-uncased")

show_tokens(text, "bert-base-cased")

"""*GPT2*"""

show_tokens(text, "gpt2")

"""*Flan-T5*"""

show_tokens(text, "google/flan-t5-small")

"""*GPT-4*"""

# The official is `tiktoken` but this the same tokenizer on the HF platform
show_tokens(text, "Xenova/gpt-4")

"""*StarCoder2*"""

# You need to request access before being able to use this tokenizer
show_tokens(text, "bigcode/starcoder2-15b")

"""*Galactica*"""

show_tokens(text, "facebook/galactica-1.3b")

"""*Phi-3*"""

show_tokens(text, "microsoft/Phi-3-mini-4k-instruct")